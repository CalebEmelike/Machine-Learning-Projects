{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datasist as ds\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Train.csv\")\n",
    "test = pd.read_csv(\"Test.csv\")\n",
    "riders = pd.read_csv(\"Riders.csv\")\n",
    "sample = pd.read_csv(\"SampleSubmission.csv\")\n",
    "datadef = pd.read_csv(\"VariableDefinitions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Id is unique.') if train['Order No'].nunique() == train.shape[0] else print('Id is not unique')\n",
    "print('Train and test sets are distinct.') if len(np.intersect1d(train['Order No'], test['Order No']))== 0 else print('oops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop\n",
    "col_2_drop = ['Arrival at Destination - Day of Month', 'Arrival at Destination - Weekday (Mo = 1)']\n",
    "train.drop(col_2_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge riders dataset to train and test datasets\n",
    "train_merged = train.merge(riders, how='left', on='Rider Id')\n",
    "test_merged = test.merge(riders, how='left', on='Rider Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of train data {}\".format(train_merged.shape))\n",
    "print(\"shape of test data {}\".format(test_merged.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.describe(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.feature_engineering.drop_redundant(train_merged)\n",
    "ds.feature_engineering.drop_redundant(test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.feature_engineering.drop_missing(train_merged, percent=80)\n",
    "ds.feature_engineering.drop_missing(test_merged, percent=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get columns\n",
    "cat_cols = ds.structdata.get_cat_feats(train_merged)\n",
    "num_cols = ds.structdata.get_num_feats(train_merged)\n",
    "\n",
    "time_cols = ['Pickup - Time', 'Arrival at Pickup - Time', \n",
    "             'Confirmation - Time', 'Placement - Time', 'Arrival at Destination - Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information from date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in time_cols:\n",
    "    train_merged[col] = pd.to_datetime(train_merged[col])\n",
    "    if col == 'Arrival at Destination - Time':\n",
    "        pass\n",
    "    else:\n",
    "        test_merged[col] = pd.to_datetime(test_merged[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get total seconds from Arrival at Destination Time and use it to calculate statistics on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged['Time from Placement to Arrival'] = (train_merged['Arrival at Destination - Time'] - train_merged['Placement - Time']).map(lambda x: x.total_seconds())\n",
    "train_merged['Time from Confirmation to Arrival'] = (train_merged['Arrival at Destination - Time'] - train_merged['Confirmation - Time']).map(lambda x: x.total_seconds())\n",
    "train_merged['Time from Arrival Pickup to Arrival'] = (train_merged['Arrival at Destination - Time'] - train_merged['Arrival at Pickup - Time']).map(lambda x: x.total_seconds())\n",
    "\n",
    "\n",
    "test_merged['Time from Placement to Arrival'] = (test_merged['Arrival at Destination - Time'] - test_merged['Placement - Time']).map(lambda x: x.total_seconds())\n",
    "test_merged['Time from Confirmation to Arrival'] = (test_merged['Arrival at Destination - Time'] - test_merged['Confirmation - Time']).map(lambda x: x.total_seconds())\n",
    "test_merged['Time from Arrival Pickup to Arrival'] = (test_merged['Arrival at Destination - Time'] - test_merged['Arrival at Pickup - Time']).map(lambda x: x.total_seconds())\n",
    "\n",
    "\n",
    "\n",
    "train_merged['Time From Placement to Confirmation'] = (train_merged['Confirmation - Time'] - train_merged['Placement - Time']).map(lambda x: x.total_seconds())\n",
    "train_merged['Time From Placement to Arrival at Pickup']  = (train_merged['Arrival at Pickup - Time'] - train_merged['Placement - Time']).map(lambda x: x.total_seconds())\n",
    "train_merged['Time From Placement to Pickup']  = (train_merged['Pickup - Time'] - train_merged['Placement - Time']).map(lambda x: x.total_seconds())\n",
    "train_merged['Time From Arrival Pickup to Pickup']  = (train_merged['Pickup - Time'] - train_merged['Arrival at Pickup - Time']).map(lambda x: x.total_seconds())\n",
    "train_merged['Time From Confirmation to Arrival Pickup']  = (train_merged['Arrival at Pickup - Time'] - train_merged['Confirmation - Time']).map(lambda x: x.total_seconds())\n",
    "\n",
    "\n",
    "test_merged['Time From Placement to Confirmation'] = (test_merged['Confirmation - Time'] - test_merged['Placement - Time']).map(lambda x: x.total_seconds())\n",
    "test_merged['Time From Placement to Arrival at Pickup']  = (test_merged['Arrival at Pickup - Time'] - test_merged['Placement - Time']).map(lambda x: x.total_seconds())\n",
    "test_merged['Time From Placement to Pickup']  = (test_merged['Pickup - Time'] - test_merged['Placement - Time']).map(lambda x: x.total_seconds())\n",
    "test_merged['Time From Arrival Pickup to Pickup']  = (test_merged['Pickup - Time'] - test_merged['Arrival at Pickup - Time']).map(lambda x: x.total_seconds())\n",
    "test_merged['Time From Confirmation to Arrival Pickup']  = (test_merged['Arrival at Pickup - Time'] - test_merged['Confirmation - Time']).map(lambda x: x.total_seconds())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get hours, minute and seconds from time columns\n",
    "cols = time_cols\n",
    "cols.remove('Arrival at Destination - Time')\n",
    "train_merged = ds.timeseries.extract_time(data=train_merged, time_cols=cols)\n",
    "test_merged = ds.timeseries.extract_time(data=test_merged, time_cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.display_missing(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.display_missing(test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing\n",
    "ds.feature_engineering.fill_missing_num(train_merged, features=['Temperature'])\n",
    "ds.feature_engineering.fill_missing_num(test_merged, features=['Temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Arrival at Destination from train dataset\n",
    "train_merged.drop('Arrival at Destination - Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Strategy\n",
    "First let's check the train test split. It helps to decide our validation strategy and gives ideas about feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_merged.groupby('Pickup - Time_hours').count()[['Rider Id']], '-o', label='train')\n",
    "plt.plot(test_merged.groupby('Pickup - Time_hours').count()[['Rider Id']], '-o', label='test')\n",
    "plt.title('Train and test hours overlap.')\n",
    "plt.legend(loc=0)\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_merged.groupby('Pickup - Day of Month').count()[['Rider Id']], '-o', label='train')\n",
    "plt.plot(test_merged.groupby('Pickup - Day of Month').count()[['Rider Id']], '-o', label='test')\n",
    "plt.title('Train and test day overlap.')\n",
    "plt.legend(loc=0)\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "ax[0].scatter(train_merged['Pickup Long'].values, train['Pickup Lat'].values,\n",
    "              color='blue', s=1, label='train', alpha=0.1)\n",
    "\n",
    "ax[1].scatter(test_merged['Pickup Long'].values, test['Pickup Lat'].values,\n",
    "              color='green', s=1, label='test', alpha=0.1)\n",
    "\n",
    "fig.suptitle('Train and test area complete overlap.')\n",
    "ax[0].legend(loc=0)\n",
    "ax[1].legend(loc=0)\n",
    "ax[0].set_ylabel('latitude')\n",
    "ax[0].set_xlabel('longitude')\n",
    "ax[1].set_xlabel('longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test set are similar, this means the test set was sampled randomly from the train set. i.e both train and test are iid. We can extend statistics calculated on train set to test set.\n",
    "\n",
    "This allows us to use unsupervised learning and feature extraction to be applied on the full data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "### PCA\n",
    "\n",
    "We use PCA to transform longitude and latitude coordinates. In this case it is not about dimension reduction since we transform 2D-> 2D. The rotation could help for decision tree splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.vstack((train_merged[['Pickup Lat', 'Pickup Long']].values,\n",
    "                    train_merged[['Destination Lat', 'Destination Long']].values,\n",
    "                    test_merged[['Pickup Lat', 'Pickup Long']].values,\n",
    "                    test_merged[['Destination Lat', 'Destination Long']].values))\n",
    "\n",
    "pca = PCA().fit(coords)\n",
    "\n",
    "#Get the new components and save in individual columns\n",
    "train_merged['pickup_pca0'] = pca.transform(train_merged[['Pickup Lat', 'Pickup Long']])[:, 0]\n",
    "train_merged['pickup_pca1'] = pca.transform(train_merged[['Pickup Lat', 'Pickup Long']])[:, 1]\n",
    "train_merged['dropoff_pca0'] = pca.transform(train_merged[['Destination Lat', 'Destination Long']])[:, 0]\n",
    "train_merged['dropoff_pca1'] = pca.transform(train_merged[['Destination Lat', 'Destination Long']])[:, 1]\n",
    "\n",
    "test_merged['pickup_pca0'] = pca.transform(test_merged[['Pickup Lat', 'Pickup Long']])[:, 0]\n",
    "test_merged['pickup_pca1'] = pca.transform(test_merged[['Pickup Lat', 'Pickup Long']])[:, 1]\n",
    "test_merged['dropoff_pca0'] = pca.transform(test_merged[['Destination Lat', 'Destination Long']])[:, 0]\n",
    "test_merged['dropoff_pca1'] = pca.transform(test_merged[['Destination Lat', 'Destination Long']])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance\n",
    "Let's calculate the distance metrics between pickup and destination points. Currently Haversine is used, geopy has another heuristics (vincenty() or great_circle()) if you prefer. we could check the Manhattan (L1) distance too.\n",
    "\n",
    "pd.DataFrame.apply() would be too slow so the haversine function is rewritten to handle arrays.\n",
    "We extraxt the middle of the path as a feature as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    a = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))\n",
    "\n",
    "\n",
    "\n",
    "train_merged['distance_haversine'] = haversine_array(train_merged['Pickup Lat'].values, train_merged['Pickup Long'].values, train_merged['Destination Lat'].values, train_merged['Destination Long'].values)\n",
    "train_merged['distance_dummy_manhattan'] = dummy_manhattan_distance(train_merged['Pickup Lat'].values, train_merged['Pickup Long'].values, train_merged['Destination Lat'].values, train_merged['Destination Long'].values)\n",
    "train_merged['direction'] = bearing_array(train_merged['Pickup Lat'].values, train_merged['Pickup Long'].values, train_merged['Destination Lat'].values, train_merged['Destination Long'].values)\n",
    "train_merged['pca_manhattan'] = np.abs(train_merged['dropoff_pca1'] - train_merged['pickup_pca1']) + np.abs(train_merged['dropoff_pca0'] - train_merged['pickup_pca0'])\n",
    "\n",
    "test_merged['distance_haversine'] = haversine_array(test_merged['Pickup Lat'].values, test_merged['Pickup Long'].values, test_merged['Destination Lat'].values, test_merged['Destination Long'].values)\n",
    "test_merged['distance_dummy_manhattan'] = dummy_manhattan_distance(test_merged['Pickup Lat'].values, test_merged['Pickup Long'].values, test_merged['Destination Lat'].values, test_merged['Destination Long'].values)\n",
    "test_merged['direction'] = bearing_array(test_merged['Pickup Lat'].values, test_merged['Pickup Long'].values, test_merged['Destination Lat'].values, test_merged['Destination Long'].values)\n",
    "test_merged['pca_manhattan'] = np.abs(test_merged['dropoff_pca1'] - test_merged['pickup_pca1']) + np.abs(test_merged['dropoff_pca0'] - test_merged['pickup_pca0'])\n",
    "\n",
    "train_merged['center_latitude'] = (train_merged['Pickup Lat'].values + train_merged['Destination Lat'].values) / 2\n",
    "train_merged['center_longitude'] = (train_merged['Pickup Long'].values + train_merged['Destination Long'].values) / 2\n",
    "test_merged['center_latitude'] = (test_merged['Pickup Lat'].values + test_merged['Destination Lat'].values) / 2\n",
    "test_merged['center_longitude'] = (test_merged['Pickup Long'].values + test_merged['Pickup Long'].values) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=30).fit(coords)\n",
    "\n",
    "train_merged['pickup_cluster'] = kmeans.predict(train_merged[['Pickup Lat', 'Pickup Long']])\n",
    "train_merged['dropoff_cluster'] = kmeans.predict(train_merged[['Destination Lat', 'Destination Long']])\n",
    "test_merged['pickup_cluster'] = kmeans.predict(test_merged[['Pickup Lat', 'Pickup Long']])\n",
    "test_merged['dropoff_cluster'] = kmeans.predict(test_merged[['Destination Lat', 'Destination Long']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1)\n",
    "ax.scatter(train['Pickup Long'], train['Pickup Lat'], s=10, lw=0,\n",
    "           c=train_merged['pickup_cluster'], cmap='tab20', alpha=0.2)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, n_train, n_test = ds.structdata.join_train_and_test(train_merged, test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['Order No'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = ['Confirmation - Day of Month', 'Confirmation - Weekday (Mo = 1)', \n",
    "                  'Arrival at Pickup - Day of Month', 'Arrival at Pickup - Weekday (Mo = 1)', \n",
    "                  'Pickup - Day of Month', 'Pickup - Weekday (Mo = 1)', 'Pickup - Time_hours',\n",
    "                 'Confirmation - Time_hours']\n",
    "\n",
    "all_data = all_data.drop(redundant_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['minute_diff_pickup_&_arrival'] = np.abs(all_data['Pickup - Time_minutes'] - all_data['Arrival at Pickup - Time_minutes'])\n",
    "all_data['seconds_diff_pickup_&_arrival'] = np.abs(all_data['Pickup - Time_seconds'] - all_data['Arrival at Pickup - Time_seconds'])\n",
    "all_data['minute_diff_confirm_&_placement'] = np.abs(all_data['Confirmation - Time_minutes'] - all_data['Placement - Time_minutes'])\n",
    "all_data['seconds_diff_confirm_&_placement'] = np.abs(all_data['Confirmation - Time_seconds'] - all_data['Placement - Time_seconds'])\n",
    "all_data['hour_diff__placement_&_arrivalpickup'] = np.abs(all_data['Placement - Time_hours'] - all_data['Arrival at Pickup - Time_hours'])\n",
    "\n",
    "# to_drop = ['Pickup - Time_minutes','Arrival at Pickup - Time_minutes', 'Pickup - Time_seconds', \n",
    "#            'Arrival at Pickup - Time_seconds', 'Confirmation - Time_minutes', 'Placement - Time_minutes',\n",
    "#           'Confirmation - Time_seconds', 'Placement - Time_seconds', 'Placement - Time_hours', 'Arrival at Pickup - Time_hours']\n",
    "\n",
    "# all_data.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['Pickup Lat', 'Pickup Long', 'Destination Lat', 'Destination Long']\n",
    "\n",
    "# for loc in locations:\n",
    "#     all_data[loc + '_binned'] = ds.feature_engineering.get_qcut(all_data, col=loc, q=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(locations, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['Rider Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Time from Pickup to Arrival')\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['User Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Time from Pickup to Arrival')\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['Rider Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Distance (KM)')\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['User Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Distance (KM)')\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['Rider Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Time from Placement to Arrival')\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['Rider Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Time from Confirmation to Arrival')\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['Rider Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Time from Arrival Pickup to Arrival')\n",
    "\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['User Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Time from Placement to Arrival')\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['User Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Time from Confirmation to Arrival')\n",
    "\n",
    "all_data = ds.feature_engineering.merge_groupby(all_data, cat_features=['User Id'], statistics=['mean'], \n",
    "                                                                    col_to_merge='Time from Arrival Pickup to Arrival')\n",
    "\n",
    "\n",
    "to_drop = ['Time from Confirmation to Arrival', 'Time from Placement to Arrival', 'Time from Arrival Pickup to Arrival']\n",
    "\n",
    "all_data.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.display_missing(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.feature_engineering.fill_missing_num(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in ['User Id', 'Rider Id']:\n",
    "    lb = LabelEncoder()\n",
    "    lb.fit(all_data[col])\n",
    "    all_data[col] = lb.transform(all_data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data, drop_first=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:n_train]\n",
    "test = all_data[n_train:]\n",
    "\n",
    "target = train['Time from Pickup to Arrival']\n",
    "\n",
    "train = train.drop(['Time from Pickup to Arrival'], axis=1)\n",
    "test = test.drop(['Time from Pickup to Arrival'], axis=1)\n",
    "\n",
    "col_names = list(train.columns ) # used for feature importance plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get pca train set\n",
    "# pca2 = PCA(n_components=30)\n",
    "# pca2.fit(train)\n",
    "\n",
    "# train_pca = pca2.transform(train)\n",
    "# test_pca = pca2.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Try feature selection\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "# lgb_model = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "#               importance_type='split', learning_rate=0.01, max_depth=9,\n",
    "#               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "#               n_estimators=500, n_jobs=-1, num_leaves=31, objective=None,\n",
    "#               random_state=2, reg_alpha=1, reg_lambda=1, silent=True,\n",
    "#               subsample=0.5, subsample_for_bin=200000, subsample_freq=0)\n",
    "\n",
    "# skbest = SelectFromModel(estimator=lgb_model).fit(train, target)\n",
    "# train_skbest = skbest.transform(train)\n",
    "# test_skbesr = skbest.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor,BaggingRegressor, VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score,train_test_split\n",
    "\n",
    "# import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale features\n",
    "scaler = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler2.fit(train)\n",
    "train = scaler2.transform(train)\n",
    "test = scaler2.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(mse):\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_best = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "              importance_type='split', learning_rate=0.01, max_depth=9,\n",
    "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "              n_estimators=1000, n_jobs=-1, num_leaves=31, objective=None,\n",
    "              random_state=2, reg_alpha=1, reg_lambda=1, silent=True,\n",
    "              subsample=0.5, subsample_for_bin=200000, subsample_freq=0)\n",
    "\n",
    "# xgb_model = xgb.XGBRegressor(n_estimators=2000, max_depth=9, n_jobs=-1, learning_rate=0.01, random_state=21)\n",
    "\n",
    "cat_model = cb.CatBoostRegressor(n_estimators=2000, silent=True)\n",
    "\n",
    "# estimators = [(\"cat\", cat_model), (\"xgb_model\", xgb_model), (\"lgb\", lgb_best)]\n",
    "estimators = [(\"cat\", cat_model), (\"lgb\", lgb_best)]\n",
    "\n",
    "vr = VotingRegressor(estimators)\n",
    "\n",
    "bgg = BaggingRegressor(cat_model,n_estimators=6, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_best.fit(X_train, y_train)\n",
    "pred = lgb_best.predict(X_test)\n",
    "\n",
    "print(rmse(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train, y_train)\n",
    "pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(rmse(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model.fit(X_train, y_train)\n",
    "pred = cat_model.predict(X_test)\n",
    "\n",
    "print(rmse(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rmse(-1 * cross_val_score(estimator=cat_model, X=train,y=target, cv=5,scoring='neg_mean_squared_error' )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr.fit(X_train, y_train)\n",
    "pred = vr.predict(X_test)\n",
    "\n",
    "print(rmse(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Hyperparameter search\n",
    "# param_grid = {'n_estimators': [2000, 3000,4000], \n",
    "#              'max_depth': [8,9],\n",
    "#              'learning_rate': [0.1,0.01] }\n",
    "\n",
    "\n",
    "# grid = GridSearchCV(estimator=cat_model, param_grid=param_grid,scoring='neg_mean_squared_error',cv=2, verbose=1, n_jobs=-1)\n",
    "# grid.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid2 = GridSearchCV(estimator=xgb_model, param_grid=param_grid,scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "# grid2.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# model_name = 'cat_model.sav'\n",
    "\n",
    "# pickle.dump(cat_model, open(model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(rmse(-1 * cross_val_score(estimator=lgb_best, X=train,y=target, cv=5,scoring='neg_mean_squared_error' )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.model.plot_feature_importance(cat_model, col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model.fit(train, target)\n",
    "final_pred = cat_model.predict(test)\n",
    "\n",
    "sample['Time from Pickup to Arrival'] = final_pred\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(\"cat_model_sendy_sat7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
